import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import re

# Configura√ß√£o do MongoDB
MONGO_URI = "mongodb://localhost:27017/"
client = MongoClient(MONGO_URI)
db = client["chatbot_db"]
vectors_collection = db["vectors"]

# Configura√ß√£o do modelo de embeddings
model = SentenceTransformer("all-MiniLM-L6-v2")

# Configura√ß√£o do FAISS
embedding_size = 384  # Tamanho do vetor do modelo
faiss_index = faiss.IndexFlatL2(embedding_size)

# Cabe√ßalhos para evitar bloqueio de scraping
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"}

def split_text_into_chunks(text, max_words=300):
    """
    Divide um texto grande em partes menores (chunks) sem cortar frases no meio.
    
    :param text: Texto a ser dividido
    :param max_words: N√∫mero m√°ximo de palavras por chunk
    :return: Lista de chunks organizados
    """
    sentences = re.split(r'(?<=[.!?])\s+', text)  # Divide o texto em frases
    chunks = []
    current_chunk = []
    current_word_count = 0

    for sentence in sentences:
        words = sentence.split()
        if current_word_count + len(words) > max_words:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            current_word_count = 0
        current_chunk.append(sentence)
        current_word_count += len(words)

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

def extract_resolution_text(url):
    """Extrai o conte√∫do da resolu√ß√£o a partir da URL."""
    response = requests.get(url, headers=HEADERS)
    if response.status_code != 200:
        print(f"‚ùå Erro ao acessar {url}")
        return None, None
    
    soup = BeautifulSoup(response.text, "html.parser")
    
    # Extrai o t√≠tulo
    title = soup.find("h1", class_="documentFirstHeading")
    title = title.get_text(strip=True) if title else "T√≠tulo Desconhecido"
    
    # Extrai o conte√∫do principal
    content_div = soup.find("div", id="content-core")
    if not content_div:
        print(f"‚ö†Ô∏è Conte√∫do n√£o encontrado em {url}")
        return title, None
    
    paragraphs = content_div.find_all("p")
    text = "\n".join([p.get_text(strip=True) for p in paragraphs])
    
    return title, text

# limita√ß√£o de resolu√ß√µes
def process_resolutions(base_url, max_resolutions=3):
    """Percorre os anos e baixa resolu√ß√µes at√© atingir o limite."""
    total_downloaded = 0

    # Acessa a p√°gina principal para obter os anos dispon√≠veis
    response = requests.get(base_url, headers=HEADERS)
    if response.status_code != 200:
        print("‚ùå Erro ao acessar a p√°gina principal de resolu√ß√µes.")
        return
    
    soup = BeautifulSoup(response.text, "html.parser")
    year_links = soup.select("a.internal-link")

    print("üîç Analisando anos dispon√≠veis...")
    for link in year_links:
        year_url = link["href"]
        if not year_url.startswith("http"):
            year_url = base_url + year_url  # Corrige URL relativa
        print(f"üìå Ano encontrado: {year_url}")

        # Acessa a p√°gina do ano espec√≠fico
        year_response = requests.get(year_url, headers=HEADERS)
        if year_response.status_code != 200:
            print(f"‚ùå Erro ao acessar {year_url}")
            continue
        
        year_soup = BeautifulSoup(year_response.text, "html.parser")
        resolution_links = year_soup.select("a.internal-link")

        print(f"üîç Resolu√ß√µes encontradas para {year_url}:")
        for res_link in resolution_links:
            res_url = res_link["href"]
            if not res_url.startswith("http"):
                res_url = base_url + res_url

            print(f"‚û°Ô∏è Verificando resolu√ß√£o: {res_url}")

            # Verifica se a resolu√ß√£o j√° est√° no banco
            if vectors_collection.find_one({"url": res_url}):
                print(f"‚ö†Ô∏è Resolu√ß√£o j√° armazenada: {res_url}")
                continue

            title, text = extract_resolution_text(res_url)
            if text:
                # üîπ Divide o texto em partes menores (chunks)
                chunks = split_text_into_chunks(text, max_words=300)

                for i, chunk in enumerate(chunks):
                    # Gerar embedding para cada chunk
                    embedding = model.encode(chunk).tolist()

                    # Salvar cada chunk no MongoDB
                    document = {
                        "title": title,
                        "chunk_index": i,
                        "text": chunk,
                        "embedding": embedding,
                        "url": res_url
                    }
                    vectors_collection.insert_one(document)

                    # Adicionar ao FAISS
                    embedding_array = np.array([embedding], dtype=np.float32)
                    faiss_index.add(embedding_array)

                    print(f"‚úÖ Chunk {i+1} de '{title}' armazenado no MongoDB e FAISS.")
                
                total_downloaded += 1


                # Se atingiu o limite, para o processo
                if total_downloaded >= max_resolutions:
                    print("üîπ Limite de resolu√ß√µes atingido. Finalizando processo.")
                    return

# URL base das resolu√ß√µes do IFAC
base_url = "https://www.ifac.edu.br/orgaos-colegiados/conselhos/consu/resolucoes"

# Executa o processo
process_resolutions(base_url)
